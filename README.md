# MÃ´ hÃ¬nh dá»±a trÃªn ROBERTa vá»›i Pre-Attention vÃ  LSTM Ä‘á»ƒ phÃ¢n loáº¡i cáº£m xÃºc cho bÃ i Ä‘Ã¡nh giÃ¡ phim

Dá»± Ã¡n nÃ y trÃ¬nh bÃ y má»™t kiáº¿n trÃºc mÃ´ hÃ¬nh lai Ä‘á»ƒ phÃ¢n loáº¡i cáº£m xÃºc cá»§a cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ phim, táº­n dá»¥ng sá»©c máº¡nh cá»§a cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ¹ng vá»›i cÃ¡c cÆ¡ cháº¿ attention vÃ  máº¡ng nÆ¡-ron há»“i quy.

## ğŸ“– Tá»•ng quan

[cite\_start]Nhiá»‡m vá»¥ lÃ  phÃ¢n loáº¡i cÃ¡c bÃ i Ä‘Ã¡nh giÃ¡ phim tá»« bá»™ dá»¯ liá»‡u **Rotten Tomatoes** thÃ nh cÃ¡c loáº¡i cáº£m xÃºc **tÃ­ch cá»±c** hoáº·c **tiÃªu cá»±c**[cite: 196, 200].

[cite\_start]MÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘á» xuáº¥t lÃ  má»™t kiáº¿n trÃºc lai káº¿t há»£p cÃ¡c thÃ nh pháº§n sau[cite: 194, 258]:

1.  [cite\_start]**Bá»™ mÃ£ hÃ³a ROBERTa**: Sá»­ dá»¥ng mÃ´ hÃ¬nh `roberta-base` Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ táº¡o ra cÃ¡c biá»ƒu diá»…n tá»« theo ngá»¯ cáº£nh[cite: 263].
2.  [cite\_start]**Lá»›p Pre-Attention**: Ãp dá»¥ng má»™t cÆ¡ cháº¿ self-attention nháº¹ lÃªn cÃ¡c tráº¡ng thÃ¡i áº©n cá»§a ROBERTa Ä‘á»ƒ lÃ m ná»•i báº­t cÃ¡c token quan trá»ng nháº¥t Ä‘á»‘i vá»›i viá»‡c phÃ¢n tÃ­ch cáº£m xÃºc[cite: 194, 265].
3.  [cite\_start]**Lá»›p Bi-directional LSTM (Bi-LSTM)**: Xá»­ lÃ½ cÃ¡c biá»ƒu diá»…n Ä‘Ã£ Ä‘Æ°á»£c trá»ng sá»‘ hÃ³a bá»Ÿi attention Ä‘á»ƒ náº¯m báº¯t cÃ¡c máº«u tuáº§n tá»± vÃ  phá»¥ thuá»™c xa trong vÄƒn báº£n[cite: 194, 267].
4.  [cite\_start]**Äáº§u phÃ¢n loáº¡i MLP**: Má»™t máº¡ng Perceptron Ä‘a lá»›p (MLP) nhá» sáº½ Ä‘Æ°a ra dá»± Ä‘oÃ¡n cáº£m xÃºc cuá»‘i cÃ¹ng[cite: 194, 268].

[cite\_start]Kiáº¿n trÃºc nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ vÆ°á»£t qua cÃ¡c phÆ°Æ¡ng phÃ¡p tinh chá»‰nh (fine-tuning) tiÃªu chuáº©n báº±ng cÃ¡ch náº¯m báº¯t hiá»‡u quáº£ hÆ¡n cÃ¡c máº«u tuáº§n tá»± vÃ  cÃ¡c tÆ°Æ¡ng tÃ¡c token chi tiáº¿t[cite: 193, 195].

## âš™ï¸ Kiáº¿n trÃºc mÃ´ hÃ¬nh

SÆ¡ Ä‘á»“ tá»•ng quan vá» kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh:

<img width="885" height="899" alt="image" src="https://github.com/user-attachments/assets/bd627378-06b6-4bf8-bcc9-570d9e32d584" />


## ğŸš€ Káº¿t quáº£

MÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ hiá»‡u suáº¥t áº¥n tÆ°á»£ng trÃªn bá»™ dá»¯ liá»‡u thá»­ nghiá»‡m (test set):

  * **Äá»™ chÃ­nh xÃ¡c (Accuracy)**: 87.15%
  * **F1-Score**: 0.8712
  * **Äá»™ chÃ­nh xÃ¡c (Precision)**: 0.8746
  * **Äá»™ thu há»“i (Recall)**: 0.8715

[cite\_start]Nhá»¯ng káº¿t quáº£ nÃ y cho tháº¥y mÃ´ hÃ¬nh vÆ°á»£t trá»™i so vá»›i viá»‡c tinh chá»‰nh ROBERTa tiÃªu chuáº©n vÃ  cÃ¡c biáº¿n thá»ƒ chá»‰ sá»­ dá»¥ng attention gáº§n Ä‘Ã¢y[cite: 196]. [cite\_start]CÃ¡c nghiÃªn cá»©u loáº¡i bá» (ablation studies) cÅ©ng xÃ¡c nháº­n ráº±ng má»—i thÃ nh pháº§n (pre-attention, LSTM, vÃ  MLP) Ä‘á»u Ä‘Ã³ng gÃ³p má»™t cÃ¡ch cÃ³ Ã½ nghÄ©a vÃ o hiá»‡u suáº¥t cuá»‘i cÃ¹ng[cite: 197].





