{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T09:20:58.406731Z",
     "iopub.status.busy": "2025-06-06T09:20:58.406547Z",
     "iopub.status.idle": "2025-06-06T09:21:32.220967Z",
     "shell.execute_reply": "2025-06-06T09:21:32.220336Z",
     "shell.execute_reply.started": "2025-06-06T09:20:58.406708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 09:21:16.957048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749201677.184259      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749201677.249551      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T09:21:32.222253Z",
     "iopub.status.busy": "2025-06-06T09:21:32.221669Z",
     "iopub.status.idle": "2025-06-06T09:21:32.231549Z",
     "shell.execute_reply": "2025-06-06T09:21:32.230829Z",
     "shell.execute_reply.started": "2025-06-06T09:21:32.222228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RobertaWithPreLSTMAttention(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"roberta-base\", num_labels=2,\n",
    "                 lstm_hidden_size=256, num_lstm_layers=1, bidirectional=True,\n",
    "                 mlp_hidden_size=256, mlp_num_layers=1):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(pretrained_model)\n",
    "        self.config = self.roberta.config\n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        self.attention_scorer = nn.Linear(self.config.hidden_size, 1)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.config.hidden_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        classifier_input_dim = lstm_hidden_size * 2 if bidirectional else lstm_hidden_size\n",
    "        \n",
    "        mlp_layers = []\n",
    "        for i in range(mlp_num_layers):\n",
    "            mlp_layers.append(nn.Linear(classifier_input_dim if i == 0 else mlp_hidden_size, mlp_hidden_size))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(0.3))\n",
    "\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(mlp_hidden_size, num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states_roberta = outputs.last_hidden_state\n",
    "\n",
    "        att_scores = self.attention_scorer(hidden_states_roberta)\n",
    "        att_weights = torch.softmax(att_scores, dim=1)\n",
    "\n",
    "        weighted_hidden_states = att_weights * hidden_states_roberta\n",
    "\n",
    "        _, (h_n, c_n) = self.lstm(weighted_hidden_states)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            pooled_output = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            pooled_output = h_n[-1, :, :]\n",
    "\n",
    "        mlp_output = self.mlp(pooled_output)\n",
    "        logits = self.classifier(self.dropout(mlp_output))\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T09:21:32.233472Z",
     "iopub.status.busy": "2025-06-06T09:21:32.233186Z",
     "iopub.status.idle": "2025-06-06T09:21:42.145342Z",
     "shell.execute_reply": "2025-06-06T09:21:42.144526Z",
     "shell.execute_reply.started": "2025-06-06T09:21:32.233448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24afa48b02d14927a907cf02cc6920d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d8ef07f0b646b49df6b7cc135d6c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6f9cdb1ef142e9a02f35feadb0a9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13dfd0fe1df44c9ea17c139285491bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8084846e606430a8ea49beace0f8707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d5c8dc476f4f7dba6fa4bf136a4706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c58af5a481c43999cc212cf0c173126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97abb5997d2048b08a3459615364a390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b4f565c23d4dd18e9c2f783246d459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e461136273a4ddaa90c7bdeff0874d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd45fb9e3b7041ae93447096e0ed6c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5918514d7f4fd8bfe86dc707ef4676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299000682b504d7d8d91288c51064fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe9dd5d26184d458ae1dc870a186cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a611abf75aec41b89ef144707f407ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.dataset['input_ids'][idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.dataset['attention_mask'][idx], dtype=torch.long)\n",
    "        }\n",
    "        item[\"labels\"] = torch.tensor(self.dataset[\"label\"][idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"label\"])\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = CustomDataset(encoded_dataset[\"train\"])\n",
    "val_dataset = CustomDataset(encoded_dataset[\"validation\"])\n",
    "test_dataset = CustomDataset(encoded_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T09:21:42.146711Z",
     "iopub.status.busy": "2025-06-06T09:21:42.146395Z",
     "iopub.status.idle": "2025-06-06T09:21:42.164699Z",
     "shell.execute_reply": "2025-06-06T09:21:42.164137Z",
     "shell.execute_reply.started": "2025-06-06T09:21:42.146679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_dataset, val_dataset,\n",
    "    epochs=5, lr=2e-5, batch_size=16,\n",
    "    patience=2, monitor=\"val_f1\", mode=\"max\",\n",
    "    weight_decay=0.01, max_grad_norm=1.0,\n",
    "    gradient_accumulation_steps=1, warmup_steps=0,\n",
    "    num_dataloader_workers=0\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    num_training_steps = (len(train_dataset) // batch_size // gradient_accumulation_steps) * epochs\n",
    "    if num_training_steps == 0:\n",
    "        num_training_steps = epochs\n",
    "\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_dataloader_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_dataloader_workers, pin_memory=True)\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    best_score = None\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs[\"loss\"]\n",
    "                logits = outputs[\"logits\"]\n",
    "            \n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item() * gradient_accumulation_steps\n",
    "            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        train_precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "        train_recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "        print(f\"Epoch {epoch+1}: Train loss={avg_train_loss:.4f}, Acc={train_acc:.4f}, F1={train_f1:.4f}, Prec={train_precision:.4f}, Rec={train_recall:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs[\"loss\"]\n",
    "                    logits = outputs[\"logits\"]\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        val_precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "        val_recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "        print(f\"Epoch {epoch+1}: Val loss={avg_val_loss:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f}, Prec={val_precision:.4f}, Rec={val_recall:.4f}\")\n",
    "\n",
    "        \n",
    "        if monitor == \"val_acc\":\n",
    "            score = val_acc\n",
    "     \n",
    "\n",
    "        if (best_score is None) or \\\n",
    "           (mode == \"min\" and score < best_score) or \\\n",
    "           (mode == \"max\" and score > best_score):\n",
    "            best_score = score\n",
    "            patience_counter = 0\n",
    "            best_model_state = deepcopy(model.state_dict())\n",
    "            print(f\">>> New best model saved at epoch {epoch+1}!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience counter: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(model, test_dataset, batch_size=32):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                logits = outputs[\"logits\"]\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    test_precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "    test_recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f} - F1: {test_f1:.4f} - \"\n",
    "          f\"Precision: {test_precision:.4f} - Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T09:21:42.165769Z",
     "iopub.status.busy": "2025-06-06T09:21:42.165523Z",
     "iopub.status.idle": "2025-06-06T11:43:30.181701Z",
     "shell.execute_reply": "2025-06-06T11:43:30.180892Z",
     "shell.execute_reply.started": "2025-06-06T09:21:42.165746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cfe5a9fc424aadbf474414f7d5d9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Training:   0%|          | 0/267 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1 - Training: 100%|██████████| 267/267 [34:54<00:00,  7.85s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=0.6960, Acc=0.5000, F1=0.3333, Prec=0.2500, Rec=0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation:   0%|          | 0/34 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1 - Validation: 100%|██████████| 34/34 [00:33<00:00,  1.02it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val loss=0.6907, Acc=0.5000, F1=0.3333, Prec=0.2500, Rec=0.5000\n",
      ">>> New best model saved at epoch 1!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training:   0%|          | 0/267 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2 - Training: 100%|██████████| 267/267 [34:50<00:00,  7.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train loss=0.5261, Acc=0.8019, F1=0.8012, Prec=0.8061, Rec=0.8019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation:   0%|          | 0/34 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2 - Validation: 100%|██████████| 34/34 [00:33<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Val loss=0.3953, Acc=0.8884, F1=0.8883, Prec=0.8899, Rec=0.8884\n",
      ">>> New best model saved at epoch 2!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training:   0%|          | 0/267 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3 - Training: 100%|██████████| 267/267 [34:48<00:00,  7.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train loss=0.3985, Acc=0.8796, F1=0.8795, Prec=0.8804, Rec=0.8796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation:   0%|          | 0/34 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3 - Validation: 100%|██████████| 34/34 [00:33<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Val loss=0.3807, Acc=0.8715, F1=0.8713, Prec=0.8731, Rec=0.8715\n",
      "Patience counter: 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training:   0%|          | 0/267 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4 - Training: 100%|██████████| 267/267 [34:57<00:00,  7.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train loss=0.3340, Acc=0.8947, F1=0.8947, Prec=0.8952, Rec=0.8947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation:   0%|          | 0/34 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4 - Validation: 100%|██████████| 34/34 [00:32<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Val loss=0.3377, Acc=0.8762, F1=0.8760, Prec=0.8777, Rec=0.8762\n",
      "Patience counter: 2/2\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaWithPreLSTMAttention(\n",
    "    num_labels=2,\n",
    "    lstm_hidden_size=128,   \n",
    "    num_lstm_layers=1,      \n",
    "    bidirectional=True,     \n",
    "    mlp_hidden_size=128,    \n",
    "    mlp_num_layers=1        \n",
    ")\n",
    "\n",
    "trained_model = train_model(\n",
    "    model, train_dataset, val_dataset,\n",
    "    epochs=10, \n",
    "    lr=2e-5,\n",
    "    batch_size=32,\n",
    "    patience=3, \n",
    "    monitor=\"val_acc\", \n",
    "    mode=\"max\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=500,\n",
    "    num_dataloader_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T12:01:33.999557Z",
     "iopub.status.busy": "2025-06-06T12:01:33.999256Z",
     "iopub.status.idle": "2025-06-06T12:02:39.051056Z",
     "shell.execute_reply": "2025-06-06T12:02:39.050251Z",
     "shell.execute_reply.started": "2025-06-06T12:01:33.999537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/34 [00:00<?, ?it/s]/tmp/ipykernel_35/2197824734.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Testing: 100%|██████████| 34/34 [01:05<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8715 - F1: 0.8712 - Precision: 0.8746 - Recall: 0.8715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(trained_model, test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
